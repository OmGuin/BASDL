{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJohrnL10cq"
      },
      "source": [
        "# Tensim BAS simulation code\n",
        "Translated to Python by: Ayush Tripathi (ayushtripathi0905@gmail.com)\\\n",
        "Modified by: Om Guin (om.guin@gmail.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJy8jMRhXk6s"
      },
      "outputs": [],
      "source": [
        "# simulation imports\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "from numpy import random\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.signal import convolve\n",
        "from scipy.signal.windows import gaussian\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkJ5oneU2XsG"
      },
      "outputs": [],
      "source": [
        "# Update when code is revised\n",
        "versionnum = 'v1p9p3'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cUtpGDZjcyK"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftm2YyFjgiZ"
      },
      "source": [
        "### Sample Gaussian-Lorentzian beam shape and display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sXgv5dyjkqm"
      },
      "outputs": [],
      "source": [
        "def GLbeam(z, y, w0, zR, yfactor, zfactor):\n",
        "  # sample GL beam shape\n",
        "  wlor2 = (w0**2)*(1+np.square(np.divide(z, zR))) # Lorentzian component\n",
        "  B = np.multiply((np.divide(w0**2, wlor2)), np.exp(-2*(np.divide(np.square(y), wlor2))))\n",
        "  return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUqwKQPa-wEI"
      },
      "source": [
        "### Sample Gaussian beam shape and display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhAniIYg99hi"
      },
      "outputs": [],
      "source": [
        "def Gbeam(z, y, w0, zR, yfactor, zfactor):\n",
        "  # sample Gaussian beam shape\n",
        "  B = np.multiply(math.exp(-2*(np.divide(np.square(y), (w0**2)))), math.exp(-2*(np.divide(np.square(z), (zR**2)))))\n",
        "  return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0_LVVrt-upP"
      },
      "source": [
        "### Histogram of Beam Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TQh5ob1H-38A"
      },
      "outputs": [],
      "source": [
        "def HistoBeam(B):\n",
        "    \"\"\"\n",
        "    Display histogram of beam shape.\n",
        "    Assume values of B normalized from 0 to 1.\n",
        "    Bin data down to 0.5% with 30 bins.\n",
        "    Superimpose a power law model of spectral index powern.\n",
        "    \"\"\"\n",
        "\n",
        "    B_flat = B.flatten()\n",
        "\n",
        "    edgemn = np.log10(0.005 * np.max(B_flat))\n",
        "    edgemx = np.log10(np.max(B_flat))\n",
        "    logedge = np.logspace(edgemn, edgemx, 30)\n",
        "\n",
        "    n, edges = np.histogram(B_flat, bins=logedge)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.loglog(logedge[:-1], n, '-ro', label='Beam Histogram')\n",
        "    plt.xlabel('Amplitude')\n",
        "    plt.ylabel('Number Events')\n",
        "\n",
        "    # over plot a basic powerlaw model with index of powern as expected\n",
        "    powern = -0.4\n",
        "\n",
        "    a = n[0]\n",
        "    b = logedge[0] ** powern\n",
        "    powerlaw_model = (a / b) * (logedge[:-1] ** powern)\n",
        "    plt.loglog(logedge[:-1], powerlaw_model, '-bx', label='Powerlaw Model')\n",
        "\n",
        "    subtitle = f\"Fit Powerlaw of index: {powern}\"\n",
        "    plt.title(\"Model Composite Beam Amplitude Histogram\\n\" + subtitle)\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMGDC_CPCDBX"
      },
      "source": [
        "### Histogram of Time Stream Burst Amplitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qDKX6ukBGO7T"
      },
      "outputs": [],
      "source": [
        "def HistoAmp(B):\n",
        "    \"\"\"\n",
        "    Display histogram of time stream data amplitudesl just like HistoBeam except with logedge manually set to generate simulated cummulative data for DL studies (24 Nov 2023).\n",
        "    Values of B normalized by \"Generate starting amplitude distributions and fraction for each species\" section.\n",
        "    Set bin data down to 0.5% with 30 bins.\n",
        "    Superimpose a power law model of spectral index powern for reference.\n",
        "    Fix for zeroes at end - extend max even further to \"catch\" everything.\n",
        "    Make sure it is adjusted for number of bins, should extend past that\n",
        "    \"\"\"\n",
        "    Nlogedge = 20  # dimension of cumm vectors / number of log bins between edgemn and edgemx\n",
        "\n",
        "    edgemn = 0.9982\n",
        "    edgemx = 3.5\n",
        "    logedge = np.logspace(edgemn, edgemx, Nlogedge)  # fixes logedge from 10 to 3162.3; last bin from edgemx(N-1) to edgemx(N)\n",
        "\n",
        "    # Compute histogram\n",
        "    n, edges = np.histogram(B.flatten(), bins=logedge)\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.figure()\n",
        "    plt.loglog(logedge[:-1], n, '-ro', label='Amplitude Histogram')  # used lowest edge of bin as bin intensity value to match dim of histogram\n",
        "    plt.xlabel('Amplitude')\n",
        "    plt.ylabel('Number Events')\n",
        "\n",
        "    # Overplot a basic power law model with index of powern as expected\n",
        "    powern = -0.4\n",
        "    a = n[0]\n",
        "    b = (logedge[0]) ** powern\n",
        "    powerlaw_model = (a / b) * (logedge[:-1] ** powern)\n",
        "    plt.loglog(logedge[:-1], powerlaw_model, '-bx', label='Powerlaw Model')\n",
        "\n",
        "    subtitle = f\"Fit Powerlaw of index: {powern}\"\n",
        "    plt.title(f\"Model Composite Time Stream Amplitude Histogram\\n{subtitle}\")\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "    is_decreasing = np.all(np.diff(n) <= 0)\n",
        "    print(is_decreasing)\n",
        "    \n",
        "    cumm_heights = n\n",
        "    return cumm_heights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert binned data (+noise) to photon TOAs using 'intra-bin' poisson process "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binwise_photon_toas(ampstreams, sampletime_sec, clock):\n",
        "    \"\"\"\n",
        "    Given an amplitude stream (photons per bin), return a flat list of photon arrival times randomly distributed within each bin.\n",
        "    \"\"\"\n",
        "    toas_matrix = []\n",
        "    n_bins, n_files = ampstreams.shape\n",
        "\n",
        "    for file_idx in range(n_files):\n",
        "        toas = []\n",
        "        for bin_idx in range(n_bins):\n",
        "            n_photons = int(round(ampstreams[bin_idx, file_idx]))\n",
        "            if n_photons > 0:\n",
        "                bin_start = bin_idx * sampletime_sec\n",
        "                times_in_bin = np.random.uniform(0, sampletime_sec, size=n_photons)\n",
        "                times = bin_start + times_in_bin\n",
        "                toas.extend(times/clock)\n",
        "        toas_matrix.append(np.sort(np.array(toas)))\n",
        "    \n",
        "    max_len = max(len(arr) for arr in toas_matrix)\n",
        "    padded_matrix = np.zeros((max_len, n_files))\n",
        "    for i, arr in enumerate(toas_matrix):\n",
        "        padded_matrix[:len(arr), i] = arr\n",
        "    \n",
        "    return padded_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N0iXPtFB_ZN"
      },
      "source": [
        "### Outputs code version information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcjwOiS0CMyD"
      },
      "outputs": [],
      "source": [
        "def Information(versionnum):\n",
        "  # Display version and hold introductory information\n",
        "\n",
        "  simfile = 'TentSim_' + versionnum\n",
        "  tstr = datetime.now() # time in UTC (4 hours ahead)\n",
        "  formatted_tstr = tstr.strftime('%d%b%y-%H%M%S')\n",
        "\n",
        "  print('version number: ' + versionnum)\n",
        "  print('Version information and details of modeling at bottom of code')\n",
        "  return simfile, formatted_tstr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dpp8kfF2VYj"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Part 1 of this code will use a Gaussian-Lorentzian beam model to simulate a series of burst distribution timestreams based on various experimental parameters. The simulated data is stored in a format consistent with raw microscope data files generated in the past (Header + Ascii columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJXSMZlm3Fyx"
      },
      "outputs": [],
      "source": [
        "NA = 1 #Effective numerical aperture\n",
        "lam1 = 5.6e-7 # wavelength (m)\n",
        "lam2 = 6e-7 # wavelength (m)\n",
        "zRscale = 3 # unitless factor to elongate z\n",
        "\n",
        "# to simulate elongation of GL beam. this is a guess right now and not optimizes.\n",
        "# getting this number wrong means the effective concentration will be off.\n",
        "\n",
        "# Set scanning and sampling time of instrument\n",
        "scanrate  = 0.0005 # optical scan rate m/s {500 um/s}\n",
        "sampletime = 0.0005 # time bin size (seconds) {500 us/sample}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVwVlBQg39eZ"
      },
      "source": [
        "### Set exictation beam and dataset parameters\n",
        "average concentration, resulting number of data points, samplong, scan speed, etc.\n",
        "\n",
        "Allow for two color channel (16 Sep 2020)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTEizwyn4KDa",
        "outputId": "d18a9712-336b-4aed-819d-431d45c42436"
      },
      "outputs": [],
      "source": [
        "#BAS beam parameters\n",
        "# Using Abbe limit: w0 = lamba/(2*NA) ~lambda/2.9\n",
        "w0_1 = lam1/(2*NA)\n",
        "w0_2 = lam2/(2*NA)\n",
        "\n",
        "# A few geometrical excitation volume estimators for reference (not critical)\n",
        "zR1 = zRscale*(math.pi*(w0_1**2)/lam1) # scaled Raleigh range\n",
        "zR2 = zRscale*(math.pi*(w0_2**2)/lam2) # scaled Raleigh range\n",
        "\n",
        "V_GL1 = ((math.pi**2)*(w0_1**2)*zR1/4) # unsaturated volume for GL beam - not used\n",
        "V_GL2 = ((math.pi**2)*(w0_2**2)*zR2/4) # unsaturated volume for GL beam - not used\n",
        "\n",
        "A_GL1 = 2*w0_1*zR1 # approx cross-sectional area in m^2; this underestimates actual detected events by a significant factor when sensitivity extends down to 0.01*max as is assumed\n",
        "A_GL2 = 2*w0_2*zR2\n",
        "\n",
        "# Set experimental conditions for time and particle concentration\n",
        "filetime = 10 # total data file time in seconds (per single file)\n",
        "particlecon = 7e-10 # average total ensemble Molar concentration; change filename\n",
        "\n",
        "\n",
        "partperm3 = particlecon*1000*6.023e23 # particles per m^3 based on concentration\n",
        "effpartspacing = (1/partperm3)**(1/3) # est. space between particles in meters based on concentration\n",
        "\n",
        "# Physical width/length of one sample in meters\n",
        "samplewidth = scanrate*sampletime # meters/sample\n",
        "numbersamples = filetime/sampletime # samples\n",
        "totalwidth = samplewidth*numbersamples # total scan length in meters\n",
        "\n",
        "# Estimate of sample volume in one sample\n",
        "effvolsample1 = A_GL1*scanrate*sampletime # est. rectangular volume m^3 of one sample during scan in x direction; this may not be same size as effective beam volume\n",
        "effvolsample2 = A_GL2*scanrate*sampletime\n",
        "\n",
        "approxpartpersamp1 = effvolsample1*partperm3 # estimate of fraction # particles per sample (not per beam volume)\n",
        "# keep in mind detection limit is beyond effective beam volume by quite a bit so more particle strikes detected than this number (accounted for later by scaling)\n",
        "approxpartpersamp2 = effvolsample2*partperm3\n",
        "\n",
        "# Use the avg fraction of particles per sample and total time to estimate the number of bursts to simulate within one effective GL beam volume\n",
        "# This will be scaled up based on user settings later. The scaling of number goes as the square of the wavelength ratio for Gaussian approximation\n",
        "neventsamples1 = round(filetime/sampletime*approxpartpersamp1) # estimated # detected in time\n",
        "neventsamples2 = round(filetime/sampletime*approxpartpersamp2)\n",
        "\n",
        "out1 = 'Simulating ' + str(neventsamples1) + ' significant bursts within primary volume in ' + str(int(filetime/sampletime)) + ' sample bins...'\n",
        "print(out1)\n",
        "\n",
        "out2 = 'Simulating ' + str(neventsamples2) + ' significant bursts wthin primary volume in ' + str(int(filetime/sampletime)) + ' sample bins...'\n",
        "print(out2)\n",
        "\n",
        "# Check that there is no where near as many events as bins (single particle limit as needed for BAS). Chosing 10x as cutoff\n",
        "if (filetime/sampletime) < (10*neventsamples1) or (filetime/sampletime) < (10*neventsamples2):\n",
        "  print(\"Too concentrated to preserve single particle requirement. Adjust code.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvGde5-EFn_Y"
      },
      "source": [
        "### Generate beam crossing ampltiudes & set number of sim files\n",
        "\n",
        "Display GL Beam Profile Model and generate beam crossing amplitudes for the given experiment parameters. These represent crossing amplitudes of all events but we will subdivide these into different particle distributions. Each file constrained have the same number of bursts so not truly sampling noise with repeats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFOw-qbJGooZ",
        "outputId": "a1851df7-a2ce-4cf6-c7a7-b69c1c82ee1f"
      },
      "outputs": [],
      "source": [
        "# Select number of runs (same sample run again) and repeats (fresh sample) for writing data files\n",
        "runfiles = 3 # must be an integer from 1-20; max of 20 run files is hardwired\n",
        "repeatfiles = 2 # must be an integer from 1-20; max of 20 repeats is hardwired\n",
        "\n",
        "totfiles = runfiles*repeatfiles\n",
        "print(\"Number of runs of same sample: \" + str(runfiles))\n",
        "print(\"Number of repeats with fresh sample: \" + str(repeatfiles))\n",
        "print(\"Number of sim files generated: \" + str(totfiles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XELy7tIwJS37",
        "outputId": "5e8b071f-5818-4a90-8c9c-29092129ec7d"
      },
      "outputs": [],
      "source": [
        "# Factor by which to expand sampling volume; same for both channels\n",
        "# Set these so that model beam amplitude histogram fits powerlaw down to a value of 1/Dynrange\n",
        "zfactor = 3.5 # z-axis sample half range (m); must be between 1 and 20, inclusive\n",
        "yfactor = 2.5 # y-axis laterial sample half range (m); must be between 1 and 20, inclusive\n",
        "\n",
        "Dynrange = 200 # Usable dynamic range of intensity measurement; must be between 1 and 1000, inclusive\n",
        "\n",
        "# determine number of sample points for scaled volume Color A\n",
        "scaledvol1 = (2*zR1*zfactor)*(2*w0_1*yfactor)*totalwidth # scaled total scan volume to sample in m^3\n",
        "nsamples1 = round(scaledvol1/(effpartspacing**3)) # est. of number of samples needed in rectangular\n",
        "\n",
        "# scaledvol; this should be over-estimate by at least zfactor*yfactor\n",
        "# compared to number needed as set by estimate above. This will serve as a library of possible events to draw from.\n",
        "print(\"Number of event samples in one run of specified volume of simulated data: \" + str(nsamples1))\n",
        "\n",
        "# determine number of sample points for scaled volume Color B\n",
        "scaledvol2 = (2*zR2*zfactor)*(2*w0_2*yfactor)*totalwidth # scaled total scan volume to sample in m^3\n",
        "nsamples2 = round(scaledvol2/(effpartspacing**3)) # est. of number of samples needed in scaledvol\n",
        "print(\"Number of event samples in one run of specified volume of simulated data: \" + str(nsamples2))\n",
        "\n",
        "# Whichever channel requires more sampling, use that to set number of points to sample. Excess sampling in more narrow beam will be <0.005*max value and discarded later\n",
        "if neventsamples1 >= neventsamples2:\n",
        "  nsamples = nsamples1; # number for oversampling\n",
        "  neventsamples = neventsamples1; # number actually needed to sample to match concentration\n",
        "  w0 = w0_1\n",
        "  zR = zR1\n",
        "else:\n",
        "  nsamples = nsamples2;\n",
        "  neventsamples = neventsamples2; # number actually needed to sample to match concentration\n",
        "  w0 = w0_2\n",
        "  zR = zR2\n",
        "\n",
        "# Sanity check that there is enough beam sampling about to happen to fill the estimated needed number of events, neventsamples, found above\n",
        "if min(neventsamples1, neventsamples2) > nsamples:\n",
        "  print(\"Not enough data generated in random beam sampling for this choice of time & sampling. Adjust code\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icXOTKPtR6QN",
        "outputId": "bc12eea6-677b-44b5-f5c8-3e8ebc5b0871"
      },
      "outputs": [],
      "source": [
        "# Sample same regions of space for both channels then discard events below set Dynamic range threshold (typical 0.005*max)\n",
        "\n",
        "# nsamples of (y, z) pairs with random placement in rectangular region set by larger beam\n",
        "# Easier than sampling region based on GL shape.\n",
        "# Shouldn't matter as long as enough sampling happens to generate values down to ~0.005, where the beam shape model is no longer valid\n",
        "\n",
        "z = (-1+2*random.rand(nsamples, totfiles))*zfactor*zR # centered about value of 0\n",
        "y = (-1+2*random.rand(nsamples, totfiles))*yfactor*w0 # centered about value of 0\n",
        "\n",
        "for chan in range(2):\n",
        "\n",
        "  # note: neventsamples is only meant as an estimate to correct number of detected bursts for a given concentration, Doesn't account for detector threshold set in analysis, etc.\n",
        "  if chan == 0:\n",
        "    w0 = w0_1\n",
        "    zR = zR1;\n",
        "    plottitle = \"Y-Z Beam Gaussian-Lorentzian Profile: Channel A\\n\" + str(lam1)\n",
        "  else:\n",
        "    w0 = w0_2\n",
        "    zR = zR2\n",
        "    plottitle = \"Y-Z Beam Gaussian-Lorentzian Profile: Channel B\\n\" + str(lam2)\n",
        "\n",
        "  #Sample each model with spatial points determined above by larger beam\n",
        "  BGL = GLbeam(z, y, w0, zR, yfactor, zfactor);\n",
        "\n",
        "  # Model experimental beam using a Gaussian-Lorentzian power law\n",
        "  # semi-emperical - seems to work though\n",
        "  BeamIndex = 2.1 # must be from 1 to 3; use 2.1 for BGL beam\n",
        "  BGL = BGL/np.max(BGL) # Force Normalize to 1\n",
        "  B = np.power(BGL, BeamIndex)\n",
        "\n",
        "  # Display composite beam shape\n",
        "  # Interpolate to show a surface plot of beam\n",
        "  znodes = np.arange(-zfactor * zR, zfactor * zR + zR * 0.01, zR * 0.01)\n",
        "  ynodes = np.arange(-yfactor * w0, yfactor * w0 + w0 * 0.01, w0 * 0.01)\n",
        "\n",
        "  # Grab 10000 random points to interpolate cause its faster\n",
        "  plotpnts = np.random.randint(0, B.shape[0], 10000)\n",
        "  points = np.column_stack((y[plotpnts, 0], z[plotpnts, 0]))\n",
        "\n",
        "  Y, Z = np.meshgrid(ynodes, znodes)\n",
        "  grid_points = np.column_stack((Y.ravel(), Z.ravel()))\n",
        "  zgrid = griddata(points, B[plotpnts, 0], grid_points, method='linear')\n",
        "\n",
        "  zgrid = zgrid.reshape(Y.shape)\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.plot_surface(Y, Z, zgrid, cmap='viridis')\n",
        "\n",
        "  ax.set_xlabel('Y axis')\n",
        "  ax.set_ylabel('Z axis')\n",
        "  ax.set_zlabel('Interpolated B')\n",
        "  plt.title(plottitle)\n",
        "  plt.show()\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  #Display histogram of resulting model beam shape\n",
        "  h=HistoBeam(B) # Beam profile normalized to a max value of 1 at this point\n",
        "\n",
        "  # Each channel has beam model with same number of points sampled in same placed in space but more narrow beam will have more amplitudes <0.005\n",
        "  #Should see that smaller amplitude events tend to have larger amplitudes in broader beam (e.g. red beam) if simple single species and same max amplitude used for both beams.\n",
        "\n",
        "  # Now select first neventsamples of the nsamples amplitudes in each beam;\n",
        "  # Recall neventsamples is estimate of number of burst events for this concentration and beam size (see first part of code)\n",
        "\n",
        "  if chan == 0:\n",
        "    B1 = B[0:neventsamples, :]\n",
        "    z1 = z[0:neventsamples, :]\n",
        "    y1 = y[0:neventsamples, :]\n",
        "  else:\n",
        "    B2 = B[0:neventsamples, :]\n",
        "    z2 = z[0:neventsamples, :]\n",
        "    y2 = y[0:neventsamples, :]\n",
        "\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsuQ5baoUoX2"
      },
      "source": [
        "### Generate starting amplitude distributions and fraction for each species\n",
        "\n",
        "Select number of species ns (maximum of 3 for now) and amplitude distribution A subscript i of each.\n",
        "\n",
        "For each burst event, structure with time, original amp, and beam crossing amp.\n",
        "\n",
        "Set number of labels for each species to be used to determine color ratio between channels.\n",
        "\n",
        "Simulated data is in counts so + integer values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zbpQJG-VFB8",
        "outputId": "4e79eff9-2863-4f1a-b3ee-6afcbfd89c2c"
      },
      "outputs": [],
      "source": [
        "distype = 'flat' # 'flat' is only option for now; '1/n' will be implemented later\n",
        "\n",
        "match distype:\n",
        "  case 'flat':\n",
        "    print(\"Fixed amplitude species\")\n",
        "    # Amplitudes per label for each species/color channel\n",
        "    # Make as many samples as possible from 1000 to 2000, just A\n",
        "    # Amplitudes per label - from 0 to 4096\n",
        "    AmpS1ChA = 1300 \n",
        "    AmpS1ChB = 1300\n",
        "    AmpS2ChA = 1300\n",
        "    AmpS2ChB = 1300\n",
        "    AmpS3ChA = 1300\n",
        "    AmpS3ChB = 1300\n",
        "\n",
        "    # Species fractions; must be from 0 to 1\n",
        "    Frac1 = 1\n",
        "    Frac2 = 0.5\n",
        "    Frac3 = 0\n",
        "\n",
        "    # Number of fluors of each of two colors on a particle; must be from 1 to 10\n",
        "    LabelNumS1ChA = 1\n",
        "    LabelNumS1ChB = 1\n",
        "    LabelNumS2ChA = 4\n",
        "    LabelNumS2ChB = 4\n",
        "    LabelNumS3ChA = 7\n",
        "    LabelNumS3ChB = 7\n",
        "\n",
        "    # Effectively a normalized mixed ratio\n",
        "    Frac1n = Frac1/(Frac1+Frac2+Frac3)\n",
        "    Frac2n = Frac2/(Frac1+Frac2+Frac3)\n",
        "    Frac3n = Frac3/(Frac1+Frac2+Frac3)\n",
        "    #cheese\n",
        "  case '1/N':\n",
        "    print(\"To be added later\")\n",
        "\n",
        "# Generate same distribution for each wavelength channel but enforce color\n",
        "# Labeling factors\n",
        "for chan in range(2):\n",
        "  if chan == 0:\n",
        "    B = B1\n",
        "  else:\n",
        "    B = B2\n",
        "\n",
        "  nB = B[:, 0].size\n",
        "  p1 = int(np.floor(Frac1n*nB))\n",
        "  p2 = int(np.floor(p1 + Frac2n*nB))\n",
        "  p3 = int(np.floor(p2 + Frac3n*nB))\n",
        "\n",
        "  Bdis = B*0 # initialize\n",
        "\n",
        "  # scale by number of fluors per molecule\n",
        "  if Frac1n > 0:\n",
        "    if chan == 1:\n",
        "      Bdis[:p1+1, :] = np.multiply(np.multiply(AmpS1ChB, B[:p1+1, :]), LabelNumS1ChB)\n",
        "    else:\n",
        "      Bdis[:p1+1, :] = np.multiply(np.multiply(AmpS1ChA, B[:p1+1, :]), LabelNumS1ChA)\n",
        "\n",
        "  if Frac2n > 0:\n",
        "    if chan == 1:\n",
        "      Bdis[p1+1:p2+1, :] = np.multiply(np.multiply(AmpS2ChB, B[p1+1:p2+1, :]), LabelNumS2ChB)\n",
        "    else:\n",
        "      Bdis[p1+1:p2+1, :] = np.multiply(np.multiply(AmpS2ChA, B[p1+1:p2+1, :]), LabelNumS2ChA)\n",
        "\n",
        "  if Frac3n > 0:\n",
        "    if chan == 1:\n",
        "      Bdis[p2+1:, :] = np.multiply(np.multiply(AmpS3ChB, B[p2+1:, :]), LabelNumS3ChB)\n",
        "    else:\n",
        "      Bdis[p2+1:, :] = np.multiply(np.multiply(AmpS3ChA, B[p2+1:, :]), LabelNumS3ChA)\n",
        "  # Track species labels per burst (0, 1, 2)\n",
        "  species_labels = np.zeros(B.shape[0], dtype=int)  # default all to species 0\n",
        "  if Frac2n > 0:\n",
        "      species_labels[p1+1:p2+1] = 1\n",
        "  if Frac3n > 0:\n",
        "      species_labels[p2+1:] = 2\n",
        "\n",
        "  if chan == 0:\n",
        "    Bdis1 = Bdis;\n",
        "  else:\n",
        "    Bdis2 = Bdis;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ug9iAXLf1PR"
      },
      "source": [
        "### Define time of events and amplitudes streams\n",
        "\n",
        "assume random strike times and all events in channel with least number of strikes occur in other channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwv4nCHVyN1X"
      },
      "source": [
        "B is of size 4598 by 6, and represents all the sampled impact points in the GL beam.\n",
        "\n",
        "Bdis is a sectioned off version of B where the first part are the impacts for species 1, second part is for species 2, and third for species 3. The size of these sections is based on the relative concentration of the species.\n",
        "\n",
        "bindex is of size 4598 by 6 and represents the times from 0 to 120000 at which events occur (randomly sampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCJjzmEhgRNA",
        "outputId": "69c572d6-9937-4ea5-ebff-3625b760e374"
      },
      "outputs": [],
      "source": [
        "# Use same random event times for both channels. length(Bdis) should equal neventsamples\n",
        "nsamplebins = round(filetime/sampletime) # number of data points in single data file\n",
        "bindex = random.randint(0, nsamplebins, size=(neventsamples, totfiles)) # select random indices for burst events in each of the datafiles to create (totfiles)\n",
        "\n",
        "for chan in range(2):\n",
        "  if chan == 0:\n",
        "    print(\">>> Working on defining time of events for channel A\")\n",
        "    Bdis = Bdis1\n",
        "    # use channel with least events to define\n",
        "  else:\n",
        "    print(\">>> Working on defining time of events for channel B\")\n",
        "    Bdis = Bdis2\n",
        "\n",
        "  ampstreams = np.zeros((nsamplebins, totfiles))\n",
        "  for i in range(totfiles):\n",
        "    ampstreams[np.squeeze(bindex[:, i]), i] = Bdis[:, i]\n",
        "  # Parallel species label matrix: same shape as ampstreams\n",
        "  \n",
        "  species_label_stream = -1 * np.ones_like(ampstreams, dtype=int)  # -1 means no burst\n",
        "\n",
        "  for i in range(totfiles):\n",
        "    indices = np.squeeze(bindex[:, i])\n",
        "    species_label_stream[indices, i] = species_labels\n",
        "\n",
        "\n",
        "  # Remove events whos amplitudes are less than 1/Dynrange\n",
        "  ampstreams[ampstreams < (max(AmpS1ChA, AmpS1ChB, AmpS2ChA, AmpS2ChB, AmpS3ChA, AmpS3ChB)/Dynrange)] = 0\n",
        "\n",
        "  # Check new number of events\n",
        "  if chan == 0:\n",
        "    neventsamp1 = sum(ampstreams>0)\n",
        "    print(\"Number of Channel A events contained in timestream: \" + str(neventsamp1))\n",
        "  else:\n",
        "    neventsamp2 = sum(ampstreams>0)\n",
        "    print(\"Number of Channel B events contained in timestream: \" + str(neventsamp2))\n",
        "\n",
        "  if chan == 0:\n",
        "    ampstreamsA = ampstreams\n",
        "  else:\n",
        "    ampstreamsB = ampstreams\n",
        "\n",
        "  # Recheck histograms\n",
        "  print(\"Check histogram of scaled and dynamic range limited data stream before noise and convolved\")\n",
        "  h=HistoAmp(ampstreams)\n",
        "  print(\"\\n\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKEWNVT-2Yo5"
      },
      "source": [
        "### Convolve to simulate beam width and sampling\n",
        "\n",
        "The coefficients of a Gaussian window are computed from the following equation: $w(n)=e^{ -\\frac{1}{2}\\left( \\frac{\\alpha n}{(L-1)/2} \\right)^2 = e^{-n^2/ (2\\sigma ^2)}}$ where -(L-1)/2 ≤ n ≤ (L-1)/2, and α is inversely proportional to the standard deviation, σ, of a Gaussian random variable. The exact correspondence with the standard deviation of a Gaussian probability density function is $\\sigma=\\frac{w0}{2}$ so $\\alpha=\\frac{(L-1)}{2\\sigma}=\\frac{(L-1)}{w0}$\n",
        "\n",
        "The 1/e2 width of the intensity profile is described by $I(r)=I_{0}\\exp \\!\\left(\\!-2{\\frac {r^{2}}{w^{2}}}\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-0mMZLg8S7_",
        "outputId": "f15eee3b-8f2f-4029-ef09-1efb8200a04b"
      },
      "outputs": [],
      "source": [
        "addspread = True  # Set to True to simulate the Gaussian spread for events\n",
        "\n",
        "if addspread:\n",
        "    print(\"Adding Gaussian spread to events...\")\n",
        "\n",
        "    # Display effect\n",
        "    max_amp = np.max(ampstreams)\n",
        "    qq = np.unravel_index(np.argmax(ampstreams, axis=None), ampstreams.shape)  # Find index of the maximal event in all runs\n",
        "    ind_row, ind_col = qq\n",
        "    bigspikebefore = ampstreams[ind_row-30 : ind_row+31, ind_col]\n",
        "\n",
        "    L = 7 # Concolve window L samples; 7-15 is reasonable; odd numbers tend to work best\n",
        "\n",
        "    # For now, use same convolving filter for both channels\n",
        "    alpha = (L-1)/(w0/(scanrate*sampletime))  # Standardly defined width vector\n",
        "    alphacorr = 1 # increase spread in time stream of a burst by correction factor (semi-emperical); default is 1.5; increase for larger effect, decrease for smaller\n",
        "\n",
        "    gfilter = gaussian(L, std=(L-1)/(2*(alpha/alphacorr)))\n",
        "\n",
        "    # Normalize the Gaussian filter to ensure it leaves event peaks with the same amplitudes but adds Gaussian spread\n",
        "    gfilter /= np.sum(gfilter)\n",
        "\n",
        "    # Convolve each column in ampstreams with the Gaussian filter\n",
        "    ampstreams_convolved = np.copy(ampstreams)\n",
        "    ampstreamsA_convolved = np.copy(ampstreamsA)\n",
        "    ampstreamsB_convolved = np.copy(ampstreamsB)\n",
        "    for i in range(ampstreams.shape[1]):\n",
        "        ampstreams_convolved[:, i] = convolve(ampstreams[:, i], gfilter, mode='same')\n",
        "        ampstreamsA_convolved[:, i] = convolve(ampstreamsA[:, i], gfilter, mode='same')\n",
        "        ampstreamsB_convolved[:, i] = convolve(ampstreamsB[:, i], gfilter, mode='same')\n",
        "    # Display effect (window of +/- 30 for display purpose only)\n",
        "\n",
        "    x_values = np.arange(-30, 31)\n",
        "    ampstreams = ampstreams_convolved\n",
        "    ampstreamsA = ampstreamsA_convolved\n",
        "    ampstreamsB = ampstreamsB_convolved\n",
        "    bigspikeafter = ampstreams[ind_row-30 : ind_row+31, ind_col]\n",
        "    plt.figure()\n",
        "    plt.plot(x_values, bigspikebefore, label='Before filter')\n",
        "    plt.plot(x_values, bigspikeafter, label='After filter')\n",
        "    plt.xlabel('sample Bins')\n",
        "    plt.ylabel('Event Amplitude (cnts)')\n",
        "    plt.title('Gaussian spread filter applied')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAIpEg9XmktZ"
      },
      "source": [
        "### Add Noise (Not tested)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PAjK9bxmtR_"
      },
      "outputs": [],
      "source": [
        "bkgrndnoise = \"Poisson\" # @param [\"None\", \"Poisson\", \"Gaussian\", \"Offset\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viVpVLdunCq3",
        "outputId": "9c8d4276-2e3b-4819-e661-e9eca2ac895e"
      },
      "outputs": [],
      "source": [
        "match bkgrndnoise:\n",
        "  case 'Poisson':\n",
        "    print(\"Adding baseline Poisson noise ChA\")\n",
        "    lambdaA = 15 # 15 is default; should be from 0-100\n",
        "    noiseamp = random.poisson(lambdaA, ampstreamsA.shape) #ampstreamsA[:, 1].shape[0] gives amount of rows, len(ampstreamsA[1]) (1D arrays do not have a .shape[0] attribute and so you need to get a length) returns columns but ampstreamsA.shape returns rows and columns, combining both of those functions\n",
        "    ampstreamsA = ampstreamsA+noiseamp\n",
        "\n",
        "    print(\"Adding baseline Poisson noise ChB\")\n",
        "    lambdaB = 15 # 15 is default; should be from 0-100\n",
        "    noiseamp = random.poisson(lambdaB, ampstreamsB.shape)\n",
        "    ampstreamsB = ampstreamsB+noiseamp\n",
        "\n",
        "  case 'Offset':\n",
        "    print(\"Adding constant offset value ChA\")\n",
        "    sigoffsetA = 15 # 15 is default; should be from 0-100\n",
        "    ampstreamsA = ampstreamsA+sigoffsetA\n",
        "\n",
        "    print(\"Adding constant offset value ChB\")\n",
        "    sigoffsetB = 15 # 15 is default; should be from 0-100\n",
        "    ampstreamsB = ampstreamsB+sigoffsetB\n",
        "\n",
        "  case 'Gaussian':\n",
        "    print(\"Adding baseline Gaussian noise ChA\")\n",
        "    meanA = 10 # 10 is default; should be from 0-50\n",
        "    sigmaA = 3 # 3 is default; should be from 0-20\n",
        "    noiseamp = random.normal(meanA, sigmaA, ampstreamsA.shape)\n",
        "    ampstreamsA = ampstreamsA+noiseamp\n",
        "    ampstreamsA[ampstreamsA < 0] = 0\n",
        "\n",
        "    print(\"Adding baseline Gaussian noise ChB\")\n",
        "    meanB = 10 # 10 is default; should be from 0-50\n",
        "    sigmaB = 3 # 3 is default; should be from 0-20\n",
        "    noiseamp = random.normal(meanB, sigmaB, ampstreamsB.shape)\n",
        "    ampstreamsB = ampstreamsB+noiseamp\n",
        "    ampstreamsB[ampstreamsB < 0] = 0\n",
        "\n",
        "signoiseB = 'None' # Options: 'None' or 'Shot'; same noise values added on each rerun of code\n",
        "match signoiseB:\n",
        "  case 'Shot':\n",
        "    print(\"Adding shot noise ChB: var = mean count\")\n",
        "    for i in range(totfiles):\n",
        "      noiseamp = random.poisson(ampstreamsB[:, i])\n",
        "      ampstreamsB[:, i] = noiseamp.squeeze()\n",
        "\n",
        "print(ampstreamsA)\n",
        "print(ampstreamsB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWGvTrgLvV9V"
      },
      "source": [
        "### Apply species dynamics (TBD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5Qpj6OovZGT"
      },
      "outputs": [],
      "source": [
        "dynam = \"none\" # @param [\"none\", \"random break\", \"random bind\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJNjnks8vkE3"
      },
      "source": [
        "### Check Inter-event interval to see if consistent with random event arrivals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVR2VdyJvrd5",
        "outputId": "5151214f-8f72-422d-bc5f-7cb901901a0b"
      },
      "outputs": [],
      "source": [
        "checkisi = True\n",
        "# This works in continuous limit but if sampling time is on the order of or larger than mean event time this will not work.\n",
        "# In this case, you need to set an amplitude threshold to limit number of events considered.\n",
        "# Since the beam model is only valid to, say, 1% consider only events larger than 1% of the maximum amplitude.\n",
        "\n",
        "if checkisi:\n",
        "  # Check interspike interval using all events; first bin will be overloaded due to limit of bin size\n",
        "  striketimesm = random.randint(1, ampstreams.ravel().shape[0], size=(bindex.ravel().shape[0], 1))\n",
        "  rtm = np.sort(striketimesm, axis=0) # chronological order of event bins\n",
        "  peakshm = np.roll(rtm, 1)\n",
        "  peakdiffm = rtm - peakshm\n",
        "\n",
        "  # For reference, model of single population distribution with same avg event rate\n",
        "  # If multiple species, events should still be uncorrelated like 1 species of higher rate\n",
        "  nhistbins = 100 # consider sampletime*nhistbins seconds of event spacing (typically 100 ms)\n",
        "  xoutm = np.arange(1, 101)\n",
        "  nm, _ = np.histogram(peakdiffm[2:-1], bins=xoutm) # hist of random samples for control\n",
        "  plt.semilogy(xoutm[:-1] * sampletime, nm, '-o', markerfacecolor='none')\n",
        "  plt.title('Random Interspike Interval of Beam Sampling Event Times')\n",
        "  plt.xlabel('Time between events')\n",
        "  plt.ylabel('Number of events')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2QRKgVC8JVm"
      },
      "source": [
        "### Allow time stream runs to be added to simulate higher concentration (TBD)\n",
        "Increasing concentration at start is limited by sample binning and uniform random distribution: doesn't yeild corect high concentration behavior. Allow co-adding all run files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-khVGEu8N-q"
      },
      "outputs": [],
      "source": [
        "coadd = False\n",
        "# adds together all runfiles for a single repeat into one of same length\n",
        "# Like adding signal prerounding to integer; binning still discrete and coadding doesn't account for thise (very simple method)\n",
        "\n",
        "if coadd and (runfiles > 1):\n",
        "  nampstreams = np.zeros((ampstreams[:, 1].shape[0], repeatfiles)) # initialize new coadded\n",
        "\n",
        "  for j in range(repeatfiles):\n",
        "    tempamp = ampstreams[:, (j)*runfiles: (j+1)*runfiles]\n",
        "    nampstreams[:, j] = np.sum(tempamp, 1)\n",
        "\n",
        "  runfiles = 1\n",
        "  ampstreams = nampstreams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert binned data (+noise) to photon TOAs using 'intra-bin' poisson process "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "artificial_clock = 82.31e-12\n",
        "toasA = binwise_photon_toas(ampstreamsA, sampletime, artificial_clock)\n",
        "toasB = binwise_photon_toas(ampstreamsB, sampletime, artificial_clock)\n",
        "print(toasA.shape)\n",
        "print(toasB.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ieb4lJP_cl"
      },
      "source": [
        "### Write simulated data to a series of files\n",
        "Include header similar to thise example:\n",
        "\n",
        "\n",
        "---\n",
        "*3/30/2020  11:39 AM*\n",
        "\n",
        "*Total aquisition time: 60.92 sec*\n",
        "\n",
        "*MT Clock: 82.31 psec*\n",
        "\n",
        "*Bin width for correlator intensity data: 500.00 usec*\n",
        "\n",
        "*Scan diameter: 0.40 mm*\n",
        "\n",
        "*Scan speed: 0.50 mm/sec*\n",
        "\n",
        "*Lasers Active: 488 @ 0.050 mW*\n",
        "\n",
        "*Laser modulation not active*\n",
        "\n",
        "*Notes:*\n",
        "\n",
        "*1/400000 dilution of 40 nm GreenFS in miliQ water*\n",
        "\n",
        "***end header***\n",
        "\n",
        "Corr_I_A          Corr_I_B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Data follows in 2 column integer text with tab seperation: *blue # coln   red # coln *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdWU4ZCFRHht",
        "outputId": "422c9143-f422-4a73-84af-d9f5793fcecc"
      },
      "outputs": [],
      "source": [
        "# Simulation parameters\n",
        "savefiles = True\n",
        "\n",
        "if savefiles:\n",
        "    data_dir = \"sim_data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    simfile, timestr = Information(versionnum)\n",
        "    newdir = os.path.join(data_dir, f\"{simfile}_{timestr}\")\n",
        "    os.makedirs(newdir, exist_ok=True)\n",
        "\n",
        "\n",
        "    GT = {\n",
        "        \"Amplitudes\":{\n",
        "            \"AmpS1ChA\": AmpS1ChA,\n",
        "            \"AmpS1ChB\": AmpS1ChB,\n",
        "            \"AmpS2ChA\": AmpS2ChA,\n",
        "            \"AmpS2ChB\": AmpS2ChB,\n",
        "            \"AmpS3ChA\": AmpS3ChA,\n",
        "            \"AmpS3ChB\": AmpS3ChB\n",
        "        },\n",
        "        \"SpeciesFractions\": {\n",
        "            \"Frac1\": Frac1,\n",
        "            \"Frac2\": Frac2,\n",
        "            \"Frac3\": Frac3\n",
        "        },\n",
        "        \"LabelNumbers\": {\n",
        "            \"LabelNumS1ChA\": LabelNumS1ChA,\n",
        "            \"LabelNumS1ChB\": LabelNumS1ChB,\n",
        "            \"LabelNumS2ChA\": LabelNumS2ChA,\n",
        "            \"LabelNumS2ChB\": LabelNumS2ChB,\n",
        "            \"LabelNumS3ChA\": LabelNumS3ChA,\n",
        "            \"LabelNumS3ChB\": LabelNumS3ChB\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(newdir, f\"{timestr}-GROUND_TRUTH.json\"), 'w') as f:\n",
        "        json.dump(GT, f, indent=4)\n",
        "\n",
        "    # Stores a parameters file with all runfile names for use with Tentsim_Preprocess.mlx\n",
        "    \n",
        "    filenameparam = os.path.join(newdir, f\"{timestr}-preparams.par\")\n",
        "    np.save(os.path.join(newdir, \"specieslabels\"), species_label_stream)\n",
        "    k = 0\n",
        "    # Opens parameter file for writing\n",
        "    with open(filenameparam, 'w') as fileIDparam:\n",
        "\n",
        "        fileIDparam.write('%%%% (maintain 5 header lines) \\n')\n",
        "        fileIDparam.write('These data represent simulated data with 3 independent populations \\n')\n",
        "        fileIDparam.write(f'These header lines can be used to explain the point of the data collection \\n')\n",
        "        fileIDparam.write('and point to other informative files with additional information. \\n')\n",
        "        fileIDparam.write('%%%% \\n\\n')\n",
        "\n",
        "\n",
        "        for i in range(1, repeatfiles + 1):\n",
        "            for j in range(1, runfiles + 1):\n",
        "\n",
        "                # Write a data file (w/o newdir prefix) for each run into the preparam file\n",
        "                \n",
        "                filenamedat = f\"{timestr}-{i}-{j}.txt\"\n",
        "\n",
        "                # Add file name to preparam file\n",
        "                fileIDparam.write(filenamedat + '\\n')\n",
        "\n",
        "                # Store data file into new dir for each run in a specific file in the new directory\n",
        "                filename = os.path.join(newdir, filenamedat)\n",
        "\n",
        "                # Open data file for writing\n",
        "                with open(filename, 'w') as fileID:\n",
        "\n",
        "                    # TAMU style header\n",
        "                    fileID.write(f'{timestr}\\n\\n')\n",
        "                    fileID.write('Total acquisition time: 1200 sec \\n')\n",
        "                    fileID.write('MT Clock: 1 sec\\n')\n",
        "                    fileID.write('Bin width for correlator intensity data: 500.00 usec \\n')\n",
        "                    fileID.write('Scan diamater: 0.40 mm \\n')\n",
        "                    fileID.write('Scan speed: 0.50 mm/sec \\n')\n",
        "                    fileID.write('Lasers Active: 488 @ 0.050 mW \\n')\n",
        "                    fileID.write('Laser modulation not active \\n')\n",
        "                    fileID.write('Notes: \\n')\n",
        "                    fileID.write('Simulated data\\n')\n",
        "                    fileID.write('***end header*** \\n\\n')\n",
        "                    fileID.write('I_A  I_B \\n')\n",
        "                    \n",
        "\n",
        "                    # Example simulated data (using toasA and toasB)\n",
        "                    colA = np.round(toasA[:, k]).astype(int)\n",
        "                    colB = np.round(toasB[:, k]).astype(int)\n",
        "\n",
        "                    max_len = max(len(colA), len(colB))\n",
        "                    paddedA = np.pad(colA, (0, max_len - len(colA)), constant_values=0)\n",
        "                    paddedB = np.pad(colB, (0, max_len - len(colB)), constant_values=0)\n",
        "                    \n",
        "                    outdat = np.column_stack((paddedA, paddedB))\n",
        "\n",
        "                    for row in outdat:\n",
        "                        fileID.write(f\"{row[0]}\\t{row[1]}\\n\")\n",
        "\n",
        "                    k = (k+1)\n",
        "\n",
        "\n",
        "    with open(filenameparam, 'a') as fileIDparam:\n",
        "        fileIDparam.write('\\n')\n",
        "        fileIDparam.write('datatype TotTime MTclock bintime driftwinA driftwinB driftwinC threshA threshB threshC corrt dofit colA colB colC\\n')\n",
        "        formatSpec = '%d %f %.2e %.6f %d %d %d %d %d %d %.3f %d %d %d %d\\n'\n",
        "        fileIDparam.write(formatSpec % (1, filetime, artificial_clock, sampletime, 0, 0, 0, 1, 1, 1, 0.003, 1, 1, 2, 0))\n",
        "\n",
        "    print(\"Files saved successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "puJohrnL10cq",
        "9cUtpGDZjcyK",
        "18lgN4FAoWbC",
        "Rcq07GXn9wg1",
        "j5xrwvs3920R",
        "n214mL7ghNbS",
        "6b9fAA1mU_Zr",
        "X4VK-Kf4VCJU",
        "MsFndtKbT7o_",
        "ZLEHFg_baZHF",
        "nO8xMZWcY0lg",
        "GOEGocIoaNN7",
        "ZN6VT5RP4vO6",
        "ytvKHlTvbPkj",
        "053TJZIYcv_6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
