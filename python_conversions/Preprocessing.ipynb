{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eebbf1c",
   "metadata": {},
   "source": [
    "# Preprocessing Striketime Code\n",
    "\n",
    " Translated to Python by: Om Guin (om.guin@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f65b5",
   "metadata": {},
   "source": [
    "Designed for new style TAMU data (event-time files). NOT backward compatable but similar algorithm. Will need to update Preparams.params file to match new input style where there can be up to three data channels A,B,C.  The col values indicate which data column the data is stored in, where 0 means no data for that channel.\n",
    "    datatype null null sampletime driftwinA driftwinB basethreshA basethreshB corrt dofit colA colB colC\n",
    "Sabreen Update  d to store results differently than before.\n",
    "\n",
    " \n",
    "Run this from the directory containing the data files\n",
    "Point of this code is to find events and their amplitudes. Input parameters for this include a minimum threshold amplitude. Currently there is the thought of handling spikes in a detected non-zero  background. To run the same spike finding algoryhtm one needs to perform a \"basethresh\" filter where all data below basethresh (a.k.a. cut threeshold in bd_175) are set to zero. This facilitates bracketing events with zeros as would be the case for the \"clean\" single particle events BAS assumes. This type of application HAS NOT BEEN TESTED. At the very least, need to confirm that adding random noise background to a single particle burst signal leads to recovery of correct distribution.\n",
    "To use code in this way will be iterative as stands. Try plotting the time stream to determine a basethresh value to place in the parameters.params file. Suggest perhaps 3 to 5 times rms of time stream. We can automate/hardcode this after some exploration. \n",
    "Output:\n",
    "datastat = 0   if no data processed for that channel\n",
    "datastat = 1   if processed w/o Gaussian fit to events (dofit=0)\n",
    "datastat = complex structure if Gaussian fits done (dofit=1)\n",
    "handles.base = 0    if no data for the channel\n",
    "handles  .base .strike .strikeamp .d    returned in structure for processed channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import correlate, correlation_lags, medfilt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import datetime\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530da12",
   "metadata": {},
   "source": [
    "# Part 1 - Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a43500",
   "metadata": {},
   "source": [
    "### Rolling window filter to remove baseline wander but retain overall mean of timestream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86489e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basedrift_data(I, driftwin, channel):\n",
    "    \"\"\"\n",
    "    % Removes a smoothed baseline from timestream but retains overall mean of\n",
    "    % timestream; Use a 50 sample median filter (slow and memory intensive but largely insensitive to spikes) \n",
    "    % followed by a 12000 sample averaging filter on that result (fast and\n",
    "    % roughly 6 second time constant with nominal 0.5 ms sampling\n",
    "    \"\"\"\n",
    "\n",
    "    medianI = np.median(I)\n",
    "    qq = I - medianI #removes median to roughly center baseline on zero\n",
    "    ksize = np.fix(driftwin*50)\n",
    "    if ksize % 2 == 0:  # Ensure odd\n",
    "        ksize += 1\n",
    "    MF = medfilt(qq, kernel_size=ksize) #sets window size and filters\n",
    "    basedrift = MF+medianI #restores original amplitude\n",
    "    basedrift = gaussian_filter1d(basedrift, sigma = 12000/6)\n",
    "\n",
    "    cI = I - basedrift\n",
    "    cI += medianI\n",
    "    cI[cI < 0] = 0\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(I, 'b', label='Original')\n",
    "    plt.plot(basedrift, 'r', label='Baseline Drift')\n",
    "    plt.xlim([0, 10000])\n",
    "    plt.ylim([0, medianI * 5])\n",
    "    plt.title(f'Original data with drift overplotted : {channel}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return cI, basedrift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284aa6e5",
   "metadata": {},
   "source": [
    "### Offset removal\n",
    "In bd_175.m this is also called \"Cut threshold\". This routine can be used to subtract a background before finding spikes and spike amps. This subtracts a smoothed drifting function from the raw data and sets any values below zero to zero. The idea is to remove an unresolved, ubiquitous, luminous background that is adding to the event amplitudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basethresh_data(I, basethresh, channel):\n",
    "    \"\"\"\n",
    "    % Used same median filter as used in Basedrift to find a baseline but is then removed. Zeroes out any data \n",
    "    % below zero value in each channel. \n",
    "    % Use a 50 sample median filter (slow and memory intensive but largely insensitive to spikes) \n",
    "    % followed by a 12000 sample averaging filter on that result (fast and\n",
    "    % roughly 6 second time constant with nominal 0.5 ms sampling\n",
    "    \"\"\"\n",
    "    medianI = np.median(I)\n",
    "    qq = I - medianI #Removes median to roughly center baseline on zero\n",
    "    ksize = np.fix(basethresh*50).astype(int)\n",
    "    if ksize%2==0:\n",
    "        ksize+=1\n",
    "    \n",
    "    MF = medfilt(qq, kernel_size=ksize) #sets window size and filters\n",
    "    baseoffset = MF+medianI #restores original amplitude\n",
    "    baseoffset = gaussian_filter1d(baseoffset, sigma = 12000/6) #SLIGHTLY DIFFERENT FROM MATLAB VERSION\n",
    " \n",
    "    cI = I - baseoffset #Remove drift but now there are negative values\n",
    "    cI[cI < 0] = 0 #zero any negative values\n",
    "\n",
    "    #Overplot data and baseline in zoomed in function\n",
    "    plt.figure()\n",
    "    plt.plot(I, 'b', label='Original')\n",
    "    plt.plot(baseoffset, 'r', label='Baseline Offset')\n",
    "    plt.xlim([0, 10000])\n",
    "    plt.ylim([0, max(medianI * 5, 10)]) #Handles cases with zero median value\n",
    "    plt.title(f'Original presubtracted data with filtered baseline offset to be SUBTRACTED : {channel}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return cI, baseoffset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef17cd5",
   "metadata": {},
   "source": [
    "### Save processed results to .npz -- example unpacking in last cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed(pre_path, pre_filename, datastatA, datastatB, datastatC, handlesA, handlesB, handlesC, peaklocA, peaklocB, peaklocC, readme_pre, versionnum): \n",
    "    #Get information about this data run and create a director for this data store\n",
    "    filebase, tstr = Information(versionnum)\n",
    "    newdir = os.path.join(pre_path, f\"{filebase}_{tstr}\")\n",
    "    os.makedirs(newdir, exist_ok=True)\n",
    "\n",
    "    save_file_path = os.path.join(newdir, f\"{filebase}{pre_filename}{tstr}.npz\")\n",
    "    np.savez_compressed(\n",
    "        save_file_path,\n",
    "        datastatA=datastatA,\n",
    "        datastatB=datastatB,\n",
    "        datastatC=datastatC,\n",
    "        filebase=filebase,\n",
    "        handlesA=handlesA,\n",
    "        handlesB=handlesB,\n",
    "        handlesC=handlesC,\n",
    "        newdir=newdir,\n",
    "        peaklocA=peaklocA,\n",
    "        peaklocB=peaklocB,\n",
    "        peaklocC=peaklocC,\n",
    "        prefilenamesave=save_file_path,\n",
    "        pre_filename=pre_filename,\n",
    "        pre_path=pre_path,\n",
    "        readmePRE=readme_pre,\n",
    "        tstr=tstr,\n",
    "        versionnum=versionnum\n",
    "    )\n",
    "    print(f\"Preprocessed data saved to: {save_file_path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aef97e",
   "metadata": {},
   "source": [
    "### Find burst location times and amplitudes (preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def burst_data(d, tbin, corrt, dofit, channel, handles):\n",
    "    \"\"\"\n",
    "    %This is original 2013 preprocessing code used in MegaMan_V1.m and bd_175\n",
    "    %Some of the commented code is left to show comparison to what was - if\n",
    "    %commented it was not doing anything even in old code\n",
    "    %Channel data are mapped to appropriate old variables like handles\n",
    "    %Input variable channel is string to label channel being processed\n",
    "    %handles.medwin=5;    Not used in bd_175 either\n",
    "    \"\"\"\n",
    "    handles['d'] = np.zeros((2, len(d)))\n",
    "    handles['d'][0, :] = np.arange(len(d)) * tbin #make time vector in seconds\n",
    "    handles['d'][1, :] = d #time data filtered by drift and basethresh is read in\n",
    "    #handles['base'] =d     Original time data unfiltered is read in\n",
    "\n",
    "    print(f\"***** Processing channel : {channel}\")\n",
    "\n",
    "    #Show original data\n",
    "    plt.figure()\n",
    "    plt.plot(handles['d'][0, :], handles['base'], label='Original') #show original time stream data\n",
    "    plt.plot(handles['d'][0, :], handles['d'][1, :], '.r', label = \"Optionally filtered\")\n",
    "    plt.title(f\"Optionally drift/baseline filtered timestream and original {channel}\", fontsize=12)\n",
    "    plt.xlabel(\"Total Concatenated Time (seconds)\")\n",
    "    plt.ylabel(\"Photon counts per time bin\")\n",
    "    plt.legend(loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    Consider using a threshold level to define when burst boundary happens\n",
    "    rather than always using zero\n",
    "\n",
    "    Set an amplitude threshold for base aof spike to 0, mean, median, rms, or\n",
    "    dynamic range (1/100) of large spike\n",
    "    \"\"\"\n",
    "    #Determine upper range of spike activity ; 30 is a heuristic parameter\n",
    "    sortbase = np.sort(handles['base'])\n",
    "    UB = np.median(sortbase[-30:])\n",
    "\n",
    "    #Determine lower bound (threshold) of trustable spike activity\n",
    "    #Default to mean for most cases\n",
    "    handles['spthreshtype'] = 'mean' # @param [\"mean\", \"zero\", \"median\", \"rms\", \"dynamicrange\"]\n",
    "    dynrange = 100\n",
    "\n",
    "    if handles['spthreshtype'] == 'zero':\n",
    "        spikethresh = 0\n",
    "        print(\"Chosen spike finding threshold is zero\")\n",
    "    elif handles['spthreshtype'] == 'mean':\n",
    "        spikethresh = 2* handles['mean']\n",
    "        print(\"Chosen spike finding threshold setting is the 2*mean of 5-sigma filtered data set\")\n",
    "    elif handles['spthrestype'] == 'median':\n",
    "        spikethresh = 2 * handles['median']\n",
    "        print(\"Chosen spike finding threshold setting is the 2*median of 5-sigma filtered data set\")\n",
    "    elif handles['spthrestype'] == 'rms':\n",
    "        spikethresh = 2 * handles['rms']\n",
    "        print(\"Chosen spike finding threshold setting is the 2*rms of 5-sigma filtered data set\")\n",
    "    elif handles['spthrestype'] == 'dynamicrange':\n",
    "        #Estimate upper bound on events in a channel\n",
    "        spikethresh = round(UB/dynrange)\n",
    "        print(f\"Applying a 1/100 dynamic range threshold : {spikethresh}\")\n",
    "        print(\"Chosen spike finding threshold setting is UB/100 of dataset\")\n",
    "\n",
    "    #Bound lower threshold has some constraints\n",
    "    if spikethresh < round(UB / dynrange): #the dynamic range is minimum possible\n",
    "        spikethresh = round(UB / dynrange)\n",
    "        print('**** OVERRIDE: Restricted to Upper Bound / 100 as lower limit ****')\n",
    "    if spikethresh < 10: # set 10 as a lower limit in all cases due to shot noise\n",
    "        spikethresh = 10\n",
    "        print('**** OVERRIDE: Restricted to 10 cnts as lower limit - arbitrary Poisson noise limit ****')\n",
    "\n",
    "    #Store parameters in structure for later use\n",
    "    handles['spthresh'] = spikethresh\n",
    "    handles['UB'] = UB\n",
    "\n",
    "    strike = np.array(d) #data imported to function (not baseline)\n",
    "\n",
    "    #Set a lower bound for spike finding below the threshold as a way of estimating the subthreshold\n",
    "    #activity later\n",
    "    print(f\"Setting lower spike acceptance threshold to: {spikethresh}\")\n",
    "    print(f\"Retaining subthreshold spikes between {spikethresh/2} and {spikethresh} in Handles structure\")\n",
    "    strike[strike < spikethresh / 2] = 0 #Set lower amplitude to find the edge of spikes\n",
    "\n",
    "    #Guarantee zeroed end points\n",
    "    strike[0:2] = 0\n",
    "    strike[-2:] = 0\n",
    "\n",
    "    while True:\n",
    "        t = np.max(np.where(strike == np.max(strike))[0]) #This could be 2nd pnt, 2nd ot last pnt or in the middle of data\n",
    "        strike[t] = -1 #tip of peak in single event set to -1\n",
    "        i = 1\n",
    "        rightend = leftend = True\n",
    "        while rightend or leftend:\n",
    "            if rightend:\n",
    "                ti_index = max(min(t + i, len(strike) - 1), 1) #because nonzero index needed\n",
    "                if abs(strike[ti_index]) > 0: #see comment above about threshold\n",
    "                    strike[ti_index] = 0\n",
    "                else:\n",
    "                    rightend = False\n",
    "            if leftend:\n",
    "                ti_index = max(min(t - i, len(strike) - 1), 1)\n",
    "                if abs(strike[ti_index]) > 0: #See comment above about threshold\n",
    "                    strike[ti_index] = 0\n",
    "                else:\n",
    "                    leftend = False\n",
    "            i += 1\n",
    "        if np.max(strike) <= 0:\n",
    "            break\n",
    "\n",
    "    handles['strike'] = -strike\n",
    "    handles['strikeamp'] = -strike * handles['d'][1, :]\n",
    "\n",
    "    # Plot spike amplitudes\n",
    "    plt.figure()\n",
    "    plt.plot(handles['d'][0, :], handles['strikeamp'], 'r.', label='Located peaks')\n",
    "    plt.plot(handles['d'][0, :], handles['d'][1, :], 'b')\n",
    "    plt.axhline(y=UB, color='k', linestyle='-')\n",
    "    plt.title(f'Threshold & Upperbound with located peaks {channel}', fontsize=12)\n",
    "    plt.xlabel('Total Concatenated Time (seconds)')\n",
    "    plt.ylabel('Photon counts per time bin')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Store some of hte spike data below the threshold for later noise and \n",
    "    # signle event prob analysis; zero these data in main propagated data\n",
    "    # variables\n",
    "    submask = handles['strikeamp'] < spikethresh\n",
    "    handles['subthreshstrike'] = handles['strike'] * submask\n",
    "    handles['subthreshstrikeamp'] = handles['strikeamp'] * submask\n",
    "    strike_mask = handles['strikeamp'] > spikethresh\n",
    "    handles['strike'] *= strike_mask\n",
    "    handles['strikeamp'] *= strike_mask\n",
    "\n",
    "    #Consider peaks away from very edge of data set; 15 bin buffer is\n",
    "    #sized for later Gaussian model fit to events\n",
    "    peakloc = np.where(handles['strikeamp'] > 0)[0]\n",
    "    peakloc = peakloc[(peakloc > 15) & (peakloc < len(handles['strikeamp']) - 15)]\n",
    "    npeaks = len(peakloc)\n",
    "\n",
    "    # Check interspike timing statistics and remove events too closely spaced\n",
    "    print(\"Checking interspike timing distribution and removing events closer than corrt\")\n",
    "    ISI_check(handles,tbin,corrt)\n",
    "    \n",
    "    #Do fit of Gaussian model to each event if dofit flag set in parameter\n",
    "    if dofit: #Hook to turn off Gausian fitting - remove datastat=1 too\n",
    "        params = {'mean': [], 'sig': [], 'A': []}\n",
    "        for i in range(npeaks):\n",
    "            #start with general spike zone\n",
    "            center = peakloc[i]\n",
    "            fitme = handles['d'][1, center - 10:center + 11]\n",
    "            dzone = int(2 * np.sum(fitme) / np.max(fitme)) #refine zone assuming Gaussian peak\n",
    "            x = np.arange(center - dzone, center + dzone + 1)\n",
    "            fitme = handles['d'][1, center - dzone:center + dzone + 1]\n",
    "            A, mu, sigma = fitgauss(fitme, x, np.max(fitme), center, 3)\n",
    "            params['mean'].append(mu)\n",
    "            params['sig'].append(sigma)\n",
    "            params['A'].append(A / np.sqrt(2 * np.pi * sigma ** 2)) #A is now amplitude\n",
    "\n",
    "        q = [(0 < sig < 10 and 0 < A < 2 * np.max(handles['strikeamp'])) for sig, A in zip(params['sig'], params['A'])]\n",
    "        datastat = {\n",
    "            'mean': [params['mean'][i] for i in range(len(q)) if q[i]],\n",
    "            'sig': [params['sig'][i] for i in range(len(q)) if q[i]],\n",
    "            'A': [params['A'][i] for i in range(len(q)) if q[i]]\n",
    "        }\n",
    "    else:\n",
    "        datastat = 1\n",
    "\n",
    "    return datastat, handles, peakloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1166346",
   "metadata": {},
   "source": [
    "### Check intervals between events (ISI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87619440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ISI_check(handles, tbin, corrt):\n",
    "    \"\"\"\n",
    "    % Uses events locations in peakloc and timestream stored in handles structure\n",
    "    % to generate interspike interval plots compared to simple single-rate\n",
    "    % poisson model. The sampling time tbin and the correlation time (~width of a\n",
    "    % burst) corrt are used to exclude events that are too close in time.\n",
    "    \"\"\"\n",
    "    nsamples = len(handles['strikeamp'].flatten()) #number of sample bins in concatenated data\n",
    "    nhistbins = 100 #consider sampletime*nhistbins seconds of event spacing (typically 100ms)\n",
    "    #Check interspike interval using generated time stream and a single\n",
    "    #population model prediction based on random times and same avg rate\n",
    "    \n",
    "    striketimesm = np.sort(np.random.randint(1, nsamples, int(np.sum(handles['strike'])))) #chronological order of event bins\n",
    "    peakshm = np.roll(striketimesm, 1)\n",
    "    peakdiffm = striketimesm - peakshm #This spacing (ISI) gives statistical measure of\n",
    "\n",
    "    #For reference, model of fsingle population distribution with same avg event rate\n",
    "    #If multiple species, events shoud still be uncorrelated like 1 species\n",
    "    #of higher rate\n",
    "    xoutm = np.arange(1, nhistbins + 1)\n",
    "    nm, _ = np.histogram(peakdiffm[1:-1], bins=xoutm) #hist of random samples for control\n",
    "    plt.figure()\n",
    "    plt.semilogy(xoutm[:-1] * tbin, nm, '-o')\n",
    "    rate = len(striketimesm) / (tbin * nsamples)\n",
    "    subtitle = f\"Random event model w/ single event rate: {rate:.4f} events/sec\"\n",
    "    plt.title(f\"Random Interspike Interval Dist Compared to Data\\n{subtitle}\")\n",
    "    plt.xlabel('Time between events')\n",
    "    plt.ylabel('Number of events')\n",
    "\n",
    "    # Do again with real data shifted for comparison; should yield same powerlaw\n",
    "    #slope; Overplot distributions\n",
    "    qq = handles['strikeamp'].flatten() > 0\n",
    "    striketimesd = np.arange(1, nsamples + 1)[qq] #need transpose to switch index order; indices where strikes occured\n",
    "    rtd = np.sort(striketimesd)\n",
    "    peakshd = np.roll(rtd, 1)\n",
    "    peakdiffd = rtd - peakshd\n",
    "    xoutd = np.arange(1, nhistbins + 1)\n",
    "    nd, _ = np.histogram(peakdiffd[1:-1], bins=xoutd) #hist of random samples for control\n",
    "    plt.semilogy(xoutd[:-1] * tbin, nd, '-ro')\n",
    "    plt.legend(['Noiseless Model', 'Simulated Data'], loc='upper right')\n",
    "\n",
    "    \"\"\"\n",
    "    %Find real data interspike intervals that don't violate correlation time\n",
    "    %parameter corrt; keep first event of two that are too close; arbitrary\n",
    "    %Would be better to keep larger amplitude event\n",
    "    \"\"\"\n",
    "    dontkeep = np.abs(peakdiffd) < round(corrt / tbin) #logical indexing\n",
    "    rr = rtd[dontkeep]\n",
    "    handles['strikeamp'][rr - 1] = 0 #zero timing violation amplitude\n",
    "    handles['strike'][rr - 1] = 0 #zero the strike flag as well\n",
    "\n",
    "    #Now show with sub correlation time events removed\n",
    "    print('-----------------------------------------')\n",
    "    ntotevents = int(np.sum(handles['strike']))\n",
    "    print(f\"Removing {len(rr)} intertime violation events from {ntotevents} total events\")\n",
    "    qq = handles['strikeamp'].flatten() > 0\n",
    "    striketimesd = np.arange(1, nsamples + 1)[qq] #need transpose to switch index order; indices where strikes occured\n",
    "    rtd = np.sort(striketimesd)\n",
    "    peakshd = np.roll(rtd, 1)\n",
    "    peakdiffd = rtd - peakshd\n",
    "    nd, _ = np.histogram(peakdiffd[1:-1], bins=xoutd) #hist of random samples for control\n",
    "    plt.semilogy(xoutd[:-1] * tbin, nd, '-go')\n",
    "\n",
    "    # Fit exponential model to distribution\n",
    "    def exp_model(x, a, b):\n",
    "        return a * np.exp(b * -1 * x)\n",
    "\n",
    "    xdata = xoutd[:-1] * tbin\n",
    "    ydata = nd\n",
    "    popt, _ = curve_fit(exp_model, xdata, ydata, p0=[1, 1])\n",
    "    plt.semilogy(xdata, exp_model(xdata, *popt))\n",
    "    plt.legend(['random control', 'Input data', 'Remove timing violations'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9795c",
   "metadata": {},
   "source": [
    "### Python equivalent of fitgauss() (MATLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dff023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitgauss(fitme, x, max_fitme, peakloc, initial_sigma):\n",
    "    def gaussian(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    # Initial guesses\n",
    "    A0 = max_fitme\n",
    "    mu0 = peakloc\n",
    "    sigma0 = initial_sigma\n",
    "    \n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian, x, fitme, p0=[A0, mu0, sigma0])\n",
    "        A, mu, sigma = popt\n",
    "    except:\n",
    "        A, mu, sigma = max_fitme, peakloc, initial_sigma\n",
    "    \n",
    "    return A, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61510a",
   "metadata": {},
   "source": [
    "### Outputs code version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7428ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Information(versionnum):\n",
    "    #Display version and hold introductory information\n",
    "    prepfile = f\"Pre_{versionnum}\"\n",
    "    tstr = datetime.now().strftime(\"%d%b%y-%H%M%S\")\n",
    "    print(f\"version number: {versionnum}\")\n",
    "    return prepfile, tstr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1785b",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9f460",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "versionnum = \"v1p1\"\n",
    "print(\"FOR TIME STAMP DATA FILES ONLY. Version information and details of modeling at bottom of mlx code ' ...' below the Information function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb4276",
   "metadata": {},
   "source": [
    "### Choose subdirectory to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser('./')  # or use an absolute path if needed\n",
    "fc.title = \"<b>Select preprocessing parameter file</b>\"\n",
    "fc.filter_pattern = ['*.par']  # Only show .par files\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrePathName, PreFileName = os.path.split(fc.selected)\n",
    "print(f\"Using preprocessing parameter file: {PrePathName}{PreFileName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdca13",
   "metadata": {},
   "source": [
    "### Read parameter file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b33f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The parameter file is assumed in same directory as data\n",
    "#Parameter file should have 5 header lines at top \n",
    "\n",
    "paramfile = os.path.join(PrePathName, PreFileName)\n",
    "#parse header for info\n",
    "with open(paramfile, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line for line in lines if line!=\"\\n\"]\n",
    "readmePRE = {}\n",
    "textdata = lines[0:len(lines)-1]\n",
    "textdata[len(textdata)-1] = textdata[len(textdata)-1].split()\n",
    "colheaders = textdata[-1]\n",
    "data = [float(x) for x in lines[-1].split()]\n",
    "readmePRE[\"data\"] = data\n",
    "readmePRE[\"textdata\"] = textdata\n",
    "readmePRE[\"colheaders\"] = colheaders\n",
    "\n",
    "preproparams = readmePRE[\"data\"]\n",
    "catfilenames = readmePRE[\"textdata\"][5:len(readmePRE[\"textdata\"])-1]\n",
    "colheader = readmePRE[\"colheaders\"]\n",
    "\n",
    "#parameters: datatype sampletime driftwinA driftwinB basethreshA basethreshB corrt dofit colA colB colC\n",
    "#some of these are obsolete. Datatype is used in case there is an update to\n",
    "#formatting. Default is 1 for current TAMU format. Note, data files\n",
    "#typically have txt header with words (say about first 13 lines).\n",
    "datatype = readmePRE[\"data\"][0] #Flag for possible changes in data formatting\n",
    "Tottime = readmePRE[\"data\"][1] #Total recording time\n",
    "MTclock = readmePRE[\"data\"][2] #TAMU photon counting board clock\n",
    "tbin = readmePRE[\"data\"][3] #Binning zine in seconds to form time series\n",
    "driftwinA = readmePRE[\"data\"][4] #integer bin factor multiplied by 2000\n",
    "driftwinB = readmePRE[\"data\"][5] #Slow drift removal\n",
    "driftwinC = readmePRE[\"data\"][6]\n",
    "basethreshA = readmePRE[\"data\"][7] #Low amplitude spike hash removal (integer cnts)\n",
    "basethreshB = readmePRE[\"data\"][8] #Does change amplitude of remaining data\n",
    "basethreshC = readmePRE[\"data\"][9]\n",
    "corrt = readmePRE[\"data\"][10] #Minimal time acceptable between events\n",
    "dofit = readmePRE[\"data\"][11] #Flag for Gaussian fit to events (usually 0)\n",
    "colA = int(readmePRE[\"data\"][12]) #Column number where A data located; 0 if no data\n",
    "colB = int(readmePRE[\"data\"][13]) #Column number where B data located; 0 if no data\n",
    "colC = int(readmePRE[\"data\"][14]) #Column number where C data located; 0 if no data\n",
    "\n",
    "numcatfiles = len(catfilenames) #photon counts in time bins\n",
    "\n",
    "I_A = [0]\n",
    "I_B = [0]\n",
    "I_C = [0]\n",
    "\n",
    "#Set up time series using tbin and total time\n",
    "print(f\"Estimated number of samples per file at MT clock rate: {Tottime / MTclock}\")\n",
    "print(f\"Estimated timestream dimension per file after requested binning time: {Tottime / tbin}\")\n",
    "\n",
    "#Determine edges of time bins for photon count per bin histogram to follow\n",
    "Nafterbin = round(Tottime / tbin)\n",
    "tedge = np.arange(0, Nafterbin + 1) * tbin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6e238",
   "metadata": {},
   "source": [
    "##### Concatenate data files into 3 color channel vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29feee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code block will take VERY long if using large amounts of data (generally from simulation)... Consider commenting plt.clf(), plt.hist(..), plt.pause(1) in each of the channel conditionals\n",
    "print('Concatenating files using single baseline offset defined in params file')\n",
    "print('Confirm mean and rms of all files similar!!')\n",
    "\n",
    "\n",
    "for i in range(numcatfiles):\n",
    "    filename = os.path.join(PrePathName, catfilenames[i].strip(\"\\n\"))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    data_start_index = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"***end header***\" in line:\n",
    "            data_start_index = i + 2\n",
    "            break\n",
    "    if data_start_index is None:\n",
    "        raise ValueError(\"Bad file... couldn't find header\")\n",
    "    \n",
    "    temp = pd.read_csv(filename, delimiter='\\t', skiprows=data_start_index+1, header=None) #Tab delimited\n",
    "    #Handle possible NaN values - should be very infrequent\n",
    "    ss = temp.mean(axis = 1) #use to find rows with NaN's then drop those rows\n",
    "    allnans = np.where(~np.isfinite(ss))[0]\n",
    "    numnans = len(allnans)\n",
    "    print(f\"Number of NaN containing rows removed: {numnans}\")\n",
    "    temp = temp.drop(index = allnans)\n",
    "\n",
    "    #Form up to 3 color channels\n",
    "    if colA != 0:\n",
    "        print(\"Add a data file for A Channel\")\n",
    "        tempA = temp.iloc[:, colA - 1].to_numpy() * MTclock * 2 #Arrival times in seconds for one file\n",
    "        #factor of 2 is a feature of the BH board\n",
    "        tempA = tempA[tempA != 0] #Remove 0's that pad the data columns\n",
    "        #Apply binning set by valiable tbin=readmePRE[\"data\"][3]\n",
    "        histA, _ = np.histogram(tempA, bins=tedge)\n",
    "        I_A.extend(histA.tolist())\n",
    "        #plt.clf() #Clear current figure\n",
    "        #plt.hist(tempA, bins=tedge)\n",
    "        #plt.pause(1)\n",
    "    if colB != 0:\n",
    "        print(\"Add a data file for B Channel\")\n",
    "        tempB = temp.iloc[:, colB - 1].to_numpy() * MTclock * 2 #Arrival times in seconds for one file\n",
    "        #factor of 2 is a feature of the BH board\n",
    "        tempB = tempB[tempB != 0] #Remove 0's that pad the data columns\n",
    "        #Apply binning set by valiable tbin=readmePRE[\"data\"][3]\n",
    "        histB, _ = np.histogram(tempB, bins=tedge)\n",
    "        I_B.extend(histB.tolist())\n",
    "        #plt.clf() #Clear current figure\n",
    "        #plt.hist(tempB, bins=tedge)\n",
    "        #plt.pause(1)\n",
    "    if colC != 0:\n",
    "        print(\"Add a data file for C Channel\")\n",
    "        tempC = temp.iloc[:, colC - 1].to_numpy() * MTclock * 2 #Arrival times in seconds for one file\n",
    "        #factor of 2 is a feature of the BH board\n",
    "        tempC = tempC[tempC != 0] #Remove 0's that pad the data columns\n",
    "        #Apply binning set by valiable tbin=readmePRE[\"data\"][3]\n",
    "        histC, _ = np.histogram(tempC, bins=tedge)\n",
    "        I_C.extend(histC.tolist())\n",
    "        #plt.clf() #Clear current figure\n",
    "        #plt.hist(tempA, bins=tedge)\n",
    "        #plt.pause(1)\n",
    "\n",
    "#Remove first null data point and initialize original timestream output and\n",
    "#find spike-free rms, mean and median for possible offset correction in BAS code\n",
    "handlesA = {}\n",
    "\n",
    "if colA!=0:\n",
    "    I_A = I_A[1:]\n",
    "    handlesA['base'] = I_A\n",
    "    rms1 = np.std(I_A, ddof=1)\n",
    "    qrms1 = I_A < 5 * rms1 # Data that isn't part of major spike activity\n",
    "    handlesA[\"rms\"] = np.std(np.array(I_A)[qrms1], ddof=1)\n",
    "    handlesA[\"median\"] = np.median(np.array(I_A)[qrms1])\n",
    "    handlesA[\"mean\"] = np.mean(np.array(I_A)[qrms1])\n",
    "else:\n",
    "    print(\"No A Channel data specified\")\n",
    "    handlesA['base'] = 0\n",
    "    datastatA = 0\n",
    "    peaklocA = 0\n",
    "\n",
    "handlesB = {}\n",
    "if colB!=0:\n",
    "    I_B = I_B[1:]\n",
    "    handlesB['base'] = I_B\n",
    "    rms1 = np.std(I_B, ddof=1)\n",
    "    qrms1 = I_B < 5 * rms1 # Data that isn't part of major spike activity\n",
    "    handlesB[\"rms\"] = np.std(np.array(I_B)[qrms1], ddof=1)\n",
    "    handlesB[\"median\"] = np.median(np.array(I_B)[qrms1])\n",
    "    handlesB[\"mean\"] = np.mean(np.array(I_B)[qrms1])\n",
    "else:\n",
    "    print(\"No B Channel data specified\")\n",
    "    handlesB['base'] = 0\n",
    "    datastatB = 0\n",
    "    peaklocB = 0\n",
    "\n",
    "handlesC = {}\n",
    "if colC!=0:\n",
    "    I_C = I_C[1:]\n",
    "    handlesC['base'] = I_C\n",
    "    rms1 = np.std(I_C, ddof=1)\n",
    "    qrms1 = I_C < 5 * rms1 # Data that isn't part of major spike activity\n",
    "    handlesC[\"rms\"] = np.std(np.array(I_C)[qrms1], ddof=1)\n",
    "    handlesC[\"median\"] = np.median(np.array(I_C)[qrms1])\n",
    "    handlesC[\"mean\"] = np.mean(np.array(I_C)[qrms1])\n",
    "else:\n",
    "    print(\"No C Channel data specified\")\n",
    "    handlesC['base'] = 0\n",
    "    datastatC = 0\n",
    "    peaklocC = 0\n",
    "\n",
    "#Apply slow drift correction to baseline (paramsfile parameter 5&6)\n",
    "if driftwinA != 0 and colA != 0:\n",
    "    I_A, basedrift = basedrift_data(I_A, driftwinA, \"A\")\n",
    "    handlesA['basedriftA'] = basedrift\n",
    "if driftwinB != 0 and colB != 0:\n",
    "    I_B, basedrift = basedrift_data(I_B, driftwinB, \"B\")\n",
    "    handlesB['basedriftB'] = basedrift\n",
    "if driftwinC != 0 and colC != 0:\n",
    "    I_C, basedrift = basedrift_data(I_C, driftwinC, 'C')\n",
    "    handlesC['basedriftC'] = basedrift\n",
    "\n",
    "#Apply baseline threshold filter (paramsfile parameters 7&8) to subtract offset\n",
    "if basethreshA != 0 and colA != 0:\n",
    "    I_A, baseoffset = basethresh_data(I_A, basethreshA, 'A')\n",
    "    handlesA['baseoffsetA'] = baseoffset\n",
    "if basethreshB != 0 and colB != 0:\n",
    "    I_B, baseoffset = basethresh_data(I_B, basethreshB, 'B')\n",
    "    handlesB['baseoffsetB'] = baseoffset\n",
    "if basethreshC != 0 and colC != 0:\n",
    "    I_C, baseoffset = basethresh_data(I_C, basethreshC, 'C')\n",
    "    handlesC['baseoffsetC'] = baseoffset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6a200",
   "metadata": {},
   "source": [
    "### Determine location and amplitudes of spikes in concatenated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if colA != 0:\n",
    "    datastatA, handlesA, peaklocA = burst_data(I_A, tbin, corrt, dofit, 'A', handlesA)\n",
    "if colB != 0:\n",
    "    datastatB, handlesB, peaklocB = burst_data(I_B, tbin, corrt, dofit, 'B', handlesB)\n",
    "if colC != 0:\n",
    "    datastatC, handlesC, peaklocC = burst_data(I_C, tbin, corrt, dofit, 'C', handlesC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fbc0d",
   "metadata": {},
   "source": [
    "### Check rate of strikes and single particle limit\n",
    "##### Use a few stats to determine if single particle limit holds for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_strikestats = True\n",
    "\n",
    "if do_strikestats:\n",
    "    #Determine some statistics about detecting spikes\n",
    "    if colA != 0:\n",
    "        print(\">>>>>>>Channel A\")\n",
    "        numbins = len(handlesA[\"strike\"])\n",
    "        numspA = np.sum(handlesA[\"strike\"])\n",
    "        print(f\"Number events above A threshold: {numspA}\")\n",
    "        #If random, prob of spikes in singnle bin and multiple adjacent bins\n",
    "        p0bin1A = (numbins - numspA) / numbins\n",
    "        p1bin1A = numspA/numbins\n",
    "        print(f\"Prob of 0 event in a bin: {p0bin1A}\")\n",
    "        print(f\"Prob of 1 event in a bin: {p1bin1A}\")\n",
    "        print(f\"Prob of 2 events in a bin based on P(n=1)^2: {p1bin1A**2}\")\n",
    "        print(\" \")\n",
    "        print(f\"Fraction of events that are double strikes {(p1bin1A**2 * numbins)/numspA}\")\n",
    "\n",
    "        #average value and prob of a subthreshold event in subthresh band\n",
    "        avgsubampA = np.mean(handlesA['subthreshstrikeamp'])\n",
    "        probsubampA = np.sum(handlesA['subthreshstrikeamp']) / numbins\n",
    "        print('avg A channel contribution of subthresh events is')\n",
    "        overampsubA = probsubampA * avgsubampA\n",
    "        print(overampsubA)\n",
    "    \n",
    "    #Prob of subthreshold events contributing to more than 10%\n",
    "    if colB != 0:\n",
    "        print('>>>>>>>Channel B')\n",
    "        numbins = len(handlesB['strike'])\n",
    "        numspB = np.sum(handlesB['strike'])\n",
    "        print(f'Number events above B threshold: {numspB}')\n",
    "\n",
    "        #If random, prob of spikes in singnle bin and multiple adjacent bins\n",
    "        p0bin1B = (numbins - numspB) / numbins\n",
    "        p1bin1B = numspB/numbins\n",
    "\n",
    "\n",
    "        print(f'Prob of 0 event in a bin: {p0bin1B}')\n",
    "        print(f'Prob of 1 event in a bin: {p1bin1B}')\n",
    "        print(f'Prob of 2 events in a bin based on P(n=1)^2: {p1bin1B ** 2}')\n",
    "        print('')\n",
    "        print(f'Fraction of events that are double strikes: {(p1bin1B ** 2 * numbins) / numspB}')\n",
    "\n",
    "        #average value and prob of a subthreshold event in subthresh band\n",
    "        avgsubampB = np.mean(handlesB['subthreshstrikeamp'])\n",
    "        probsubampB = np.sum(handlesB['subthreshstrike']) / numbins\n",
    "        print('avg B channel contribution of subthresh events is')\n",
    "        overampsubB = probsubampB * avgsubampB\n",
    "        print(overampsubB)\n",
    "\n",
    "    if colC != 0:\n",
    "        print('>>>>>>>Channel C')\n",
    "        numbins = len(handlesC['strike'])\n",
    "        numspC = np.sum(handlesC['strike'])\n",
    "        print(f'Number events above C threshold: {numspC}')\n",
    "        #If random, prob of spikes in singnle bin and multiple adjacent bins\n",
    "        p0bin1C = (numbins - numspC) / numbins\n",
    "        p1bin1C = numspC/numbins\n",
    "\n",
    "        print(f'Prob of 0 event in a bin: {p0bin1C}')\n",
    "        print(f'Prob of 1 event in a bin: {p1bin1C}')\n",
    "        print(f'Prob of 2 events in a bin based on P(n=1)^2: {p1bin1C ** 2}')\n",
    "        print('')\n",
    "        print(f'Fraction of events that are double strikes: {(p1bin1C ** 2 * numbins) / numspC}')\n",
    "\n",
    "        #If random, prob of spikes in singnle bin and multiple adjacent bins\n",
    "        avgsubampC = np.mean(handlesC['subthreshstrikeamp'])\n",
    "        probsubampC = np.sum(handlesC['subthreshstrike']) / numbins\n",
    "        print('avg C channel contribution of subthresh events is')\n",
    "        overampsubC = probsubampC * avgsubampC\n",
    "        print(overampsubC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd3900",
   "metadata": {},
   "source": [
    "### Cross-correlation of datasets\n",
    "##### Determine fraction of events common to both channels using raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_xcorr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_xcorr(x, y, z):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    x = x - np.mean(x)\n",
    "    y = y - np.mean(y)\n",
    "\n",
    "    corr = correlate(x, y, mode=\"full\")\n",
    "    lags = correlation_lags(len(x), len(y), mode=\"full\")\n",
    "\n",
    "    norm_factor = np.std(x, ddof=1) * np.std(y, ddof=1) * len(x)\n",
    "    if norm_factor == 0:\n",
    "        return lags, np.zeros_like(corr)  # Avoid divide by zero\n",
    "    corr /= norm_factor\n",
    "\n",
    "    return lags, corr\n",
    "\n",
    "if colA * colB * do_xcorr != 0:\n",
    "    print(\"Both channels active: finding fractional overlap of events ...\")\n",
    "    #Find typical upper bound using 10 largest strikes\n",
    "    lags, c = normalized_xcorr(handlesA['base'], handlesB['base'], 5)\n",
    "    plt.figure()\n",
    "    plt.stem(lags, c)\n",
    "    plt.title(\"Normalized Cross-correlation of RAW SIGNAL vs bin lag\")\n",
    "\n",
    "    lags, c = normalized_xcorr(handlesA[\"strike\"], handlesB[\"strike\"], 5)\n",
    "    plt.figure()\n",
    "    plt.stem(lags, c)\n",
    "    plt.title(\"Normalized Cross-correlation of EVENT TIMES vs bin lag\")\n",
    "\n",
    "    sortA = np.sort(handlesA['strikeamp'])[::-1] #sort descending\n",
    "    sortB = np.sort(handlesB['strikeamp'])[::-1]\n",
    "    UBA = np.median(sortA[:10])\n",
    "    UBB = np.median(sortB[:10])\n",
    "    AQ1 = handlesA['strikeamp'] > 0.5 * UBA\n",
    "    BQ1 = handlesB['strikeamp'] > 0.5 * UBB\n",
    "\n",
    "    Atemp1 = handlesA['strike'] * AQ1\n",
    "    Btemp1 = handlesB['strike'] * AQ1\n",
    "    plt.figure()\n",
    "    lags, c = normalized_xcorr(Atemp1, Btemp1, 5)\n",
    "    plt.stem(lags, c)\n",
    "\n",
    "    Atemp2 = handlesA['strike'] * BQ1\n",
    "    Btemp2 = handlesB['strike'] * BQ1\n",
    "    plt.figure()\n",
    "    lags, c = normalized_xcorr(Atemp2, Btemp2, 5)\n",
    "    plt.stem(lags, c)\n",
    "    plt.legend(['All events', 'Top 50%; A-triggered', 'Top 50%; B-triggered'])\n",
    "\n",
    "    numberA = np.sum(handlesA['strike'][AQ1])\n",
    "    numberB = np.sum(handlesB['strike'][BQ1])\n",
    "\n",
    "    print('Event zero lag A-triggered, B-triggered, and auto-correlation AA, BB = 1 check')\n",
    "    fracoverlapAB = np.sum(handlesA['strike'] * handlesB['strike'] * AQ1) / numberA\n",
    "    fracoverlapBA = np.sum(handlesB['strike'] * handlesA['strike'] * BQ1) / numberB\n",
    "    fracoverlapAA = np.sum(handlesA['strike'] * handlesA['strike'] * AQ1) / numberA\n",
    "    fracoverlapBB = np.sum(handlesB['strike'] * handlesB['strike'] * BQ1) / numberB\n",
    "\n",
    "    handlesA['overlapAB'] = fracoverlapAB\n",
    "    handlesB['overlapAB'] = fracoverlapAB\n",
    "\n",
    "if colC * colB * do_xcorr != 0:\n",
    "    print(\"Both channels active: finding fractional overlap of events ...\")\n",
    "    #Find typical upper bound using 10 largest strikes\n",
    "    lags, c = normalized_xcorr(handlesC['base'], handlesB['base'], 5)\n",
    "    plt.figure(); plt.stem(lags, c); plt.title(\"X-Corr RAW: C vs B\")\n",
    "\n",
    "    lags, c = normalized_xcorr(handlesC['strike'], handlesB['strike'], 5)\n",
    "    plt.figure(); plt.stem(lags, c); plt.title(\"X-Corr EVENTS: C vs B\")\n",
    "\n",
    "    sortC = np.sort(handlesC['strikeamp'])[::-1] #sort descending\n",
    "    sortB = np.sort(handlesB['strikeamp'])[::-1]\n",
    "    UBC = np.median(sortC[:10])\n",
    "    UBB = np.median(sortB[:10])\n",
    "    CQ1 = handlesC['strikeamp'] > 0.5 * UBC\n",
    "    BQ1 = handlesB['strikeamp'] > 0.5 * UBB\n",
    "\n",
    "    Ctemp1 = handlesC['strike'] * AQ1\n",
    "    Btemp1 = handlesB['strike'] * AQ1\n",
    "    plt.figure(); lags, c = normalized_xcorr(Ctemp1, Btemp1, 5); plt.stem(lags, c)\n",
    "\n",
    "    Ctemp2 = handlesC['strike'] * BQ1\n",
    "    Btemp2 = handlesB['strike'] * BQ1\n",
    "    plt.figure(); lags, c = normalized_xcorr(Ctemp2, Btemp2, 5); plt.stem(lags, c)\n",
    "\n",
    "    plt.legend(['All events', 'Top 50%; C-triggered', 'Top 50%; B-triggered'])\n",
    "\n",
    "    numberC = np.sum(handlesC['strike'][CQ1])\n",
    "    numberB = np.sum(handlesB['strike'][BQ1])\n",
    "\n",
    "    fracoverlapCB = np.sum(handlesC['strike'] * handlesB['strike'] * CQ1) / numberC\n",
    "    fracoverlapBC = np.sum(handlesB['strike'] * handlesC['strike'] * BQ1) / numberB\n",
    "    handlesC['overlapCB'] = fracoverlapCB\n",
    "    handlesB['overlapCB'] = fracoverlapCB\n",
    "\n",
    "if colA * colC * do_xcorr != 0:\n",
    "    print(\"Both channels active: finding fractional overlap of events ...\")\n",
    "    #Find typical upper bound using 10 largest strikes\n",
    "    lags, c = normalized_xcorr(handlesA['base'], handlesC['base'], 5)\n",
    "    plt.figure(); plt.stem(lags, c); plt.title(\"X-Corr RAW: A vs C\")\n",
    "\n",
    "    lags, c = normalized_xcorr(handlesA['strike'], handlesC['strike'], 5)\n",
    "    plt.figure(); plt.stem(lags, c); plt.title(\"X-Corr EVENTS: A vs C\")\n",
    "\n",
    "    sortA = np.sort(handlesA['strikeamp'])[::-1] #sort descending\n",
    "    sortC = np.sort(handlesC['strikeamp'])[::-1]\n",
    "    UBA = np.median(sortA[:10])\n",
    "    UBC = np.median(sortC[:10])\n",
    "    AQ1 = handlesA['strikeamp'] > 0.5 * UBA\n",
    "    CQ1 = handlesC['strikeamp'] > 0.5 * UBC\n",
    "\n",
    "    Atemp1 = handlesA['strike'] * AQ1\n",
    "    Ctemp1 = handlesC['strike'] * AQ1\n",
    "    plt.figure(); lags, c = normalized_xcorr(Atemp1, Ctemp1, 5); plt.stem(lags, c)\n",
    "\n",
    "    Atemp2 = handlesA['strike'] * CQ1\n",
    "    Ctemp2 = handlesC['strike'] * CQ1\n",
    "    plt.figure(); lags, c = normalized_xcorr(Atemp2, Ctemp2, 5); plt.stem(lags, c)\n",
    "\n",
    "    plt.legend(['All events', 'Top 50%; A-triggered', 'Top 50%; C-triggered'])\n",
    "\n",
    "    numberA = np.sum(handlesA['strike'][AQ1])\n",
    "    numberC = np.sum(handlesC['strike'][CQ1])\n",
    "\n",
    "    fracoverlapAC = np.sum(handlesA['strike'] * handlesC['strike'] * AQ1) / numberA\n",
    "    fracoverlapCA = np.sum(handlesC['strike'] * handlesA['strike'] * CQ1) / numberC\n",
    "    handlesA['overlapAC'] = fracoverlapAC\n",
    "    handlesC['overlapAC'] = fracoverlapAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d476c",
   "metadata": {},
   "source": [
    "### Save results to .npz file -- example opening in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "savebasresults = True\n",
    "if savebasresults:\n",
    "    savepredone = save_preprocessed(\n",
    "        PrePathName, \n",
    "        PreFileName,\n",
    "        datastatA,\n",
    "        datastatB,\n",
    "        datastatC,\n",
    "        handlesA,\n",
    "        handlesB,\n",
    "        handlesC,\n",
    "        peaklocA,\n",
    "        peaklocB,\n",
    "        peaklocC,\n",
    "        readmePRE,\n",
    "        versionnum\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a813ae",
   "metadata": {},
   "source": [
    "#### Example viewing of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a87d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(np.load(savepredone, allow_pickle=True))\n",
    "\n",
    "\n",
    "for key, val in data.items():\n",
    "    if isinstance(val, np.ndarray):\n",
    "        if val.dtype == object and val.size == 1:\n",
    "            val = val.item()\n",
    "        \n",
    "        if isinstance(val, dict):\n",
    "            print(f\"* {key} (dict):\")\n",
    "            for subkey, subval in val.items():\n",
    "                dtype = type(subval).__name__\n",
    "                if isinstance(subval, (int, float, str, bool)):\n",
    "                    print(f\"   - {subkey}: {dtype}, value={subval}\")\n",
    "                else:\n",
    "                    shape = np.shape(subval) if hasattr(subval, 'shape') else 'scalar'\n",
    "                    print(f\"   - {subkey}: {dtype}, shape={shape}\")\n",
    "        else:\n",
    "            dtype = val.dtype.name\n",
    "            shape = val.shape\n",
    "            if val.shape == () or val.size == 1:\n",
    "                print(f\"* {key} (ndarray scalar): dtype={dtype}, value={val.item()}\")\n",
    "            else:\n",
    "                print(f\"* {key} (ndarray): dtype={dtype}, shape={shape}\")\n",
    "    else:\n",
    "        dtype = type(val).__name__\n",
    "        print(f\"* {key} ({dtype}): value={val}\")\n",
    "\n",
    "\n",
    "#handlesA = data[\"handlesA\"].item()\n",
    "#strikeamp = handlesA[\"strikeamp\"]\n",
    "#np.save(\"strikeamp.npy\", strikeamp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
